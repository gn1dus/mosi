{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gn1dus/mosi/blob/main/%D0%9B%D0%90%D0%91_2_1%D1%87%D0%B0%D1%81%D1%82%D1%8C_ipynb%22.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Загружаем библиотеки"
      ],
      "metadata": {
        "id": "2SLkDLGaT06H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import nltk\n",
        "import string\n",
        "import numpy as np\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer"
      ],
      "metadata": {
        "id": "fagntsSETxG3"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Загрузка необходимых ресурсов NLTK"
      ],
      "metadata": {
        "id": "tB9jiLKGT67e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download(\"punkt\", force=True)\n",
        "nltk.download(\"stopwords\", force=True)\n",
        "nltk.download(\"wordnet\", force=True)\n",
        "nltk.download('omw-1.4', force=True)\n",
        "nltk.download('punkt_tab', force=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1yrp-1sjTzUO",
        "outputId": "b8bdae0e-388d-4caf-cd98-0f5127448b98"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Приведение текста к нижнему регистру и удаление знаков препинания."
      ],
      "metadata": {
        "id": "0wl-OZFTT9im"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_text(text: str) -> str:\n",
        "    text = text.lower()\n",
        "    text = re.sub(r\"[^\\w\\s]\", \"\", text)  # Удаляем знаки препинания\n",
        "    return text"
      ],
      "metadata": {
        "id": "eosy_XMzUBXu"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Токенизация, удаление стоп-слов и лемматизация."
      ],
      "metadata": {
        "id": "Kv1zTs2lUHse"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize_and_lemmatize(text: str) -> list[str]:\n",
        "    stop_words = set(stopwords.words(\"english\"))\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    words = word_tokenize(text)\n",
        "    words = [lemmatizer.lemmatize(word) for word in words if word not in stop_words]\n",
        "    return words"
      ],
      "metadata": {
        "id": "0cu01CebWihg"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Построение словаря уникальных слов с индексами."
      ],
      "metadata": {
        "id": "n82FBWmOUQ9-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def build_vocabulary(corpus: list[str]) -> dict:\n",
        "    vocab = {}\n",
        "    for text in corpus:\n",
        "        for word in tokenize_and_lemmatize(text):\n",
        "            if word not in vocab:\n",
        "                vocab[word] = len(vocab)\n",
        "    return vocab"
      ],
      "metadata": {
        "id": "dYjRh029UTN_"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Реализация Bag of Words (BoW)."
      ],
      "metadata": {
        "id": "ppuDtPM9UWgH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def bag_of_words(corpus: list[str], vocab: dict) -> np.ndarray:\n",
        "    bow_matrix = np.zeros((len(corpus), len(vocab)))\n",
        "    for i, text in enumerate(corpus):\n",
        "        for word in tokenize_and_lemmatize(text):\n",
        "            if word in vocab:\n",
        "                bow_matrix[i][vocab[word]] += 1\n",
        "    return bow_matrix"
      ],
      "metadata": {
        "id": "Vg58rbt7UYZP"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Вычисление TF (термин-фреквенции) на основе BoW матрицы."
      ],
      "metadata": {
        "id": "i-wg6JrJUc-P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_tf(bow_matrix: np.ndarray) -> np.ndarray:\n",
        "    return bow_matrix / np.maximum(bow_matrix.sum(axis=1, keepdims=True), 1)"
      ],
      "metadata": {
        "id": "fnp-NVwxUaO-"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " Вычисление IDF (обратной документной частоты)."
      ],
      "metadata": {
        "id": "AcDTps3TUjI_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_idf(bow_matrix: np.ndarray) -> np.ndarray:\n",
        "    num_docs = bow_matrix.shape[0]\n",
        "    doc_freq = np.count_nonzero(bow_matrix, axis=0)\n",
        "    return np.log10(num_docs / np.maximum(doc_freq, 1))"
      ],
      "metadata": {
        "id": "xwZLMg0AUlsX"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Вычисление TF-IDF матрицы на основе TF и IDF."
      ],
      "metadata": {
        "id": "W4czJZpTUnqP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_tfidf(bow_matrix: np.ndarray) -> np.ndarray:\n",
        "    tf = compute_tf(bow_matrix)\n",
        "    idf = compute_idf(bow_matrix)\n",
        "    return tf * idf"
      ],
      "metadata": {
        "id": "8FUMx3mrUsVn"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Токенизация всех ASCII-символов."
      ],
      "metadata": {
        "id": "63ig3BCjUtDX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def ascii_tokenizer(text: str) -> list[str]:\n",
        "    return [char for char in text if char in string.printable]"
      ],
      "metadata": {
        "id": "zK4UEqQdUvKP"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Векторизация ASCII-символов (перевод символов в их ASCII-коды)."
      ],
      "metadata": {
        "id": "hhqxBos-VWW_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def ascii_vectorizer(text: str) -> list[int]:\n",
        "    return [ord(char) for char in text if char in string.printable]"
      ],
      "metadata": {
        "id": "95-JEGLgUzZv"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Векторизация текста через индексацию уникальных токенов."
      ],
      "metadata": {
        "id": "px1nzA3lU9TX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def vectorize(tokens: list[str]) -> list[int]:\n",
        "    dict_vectors = {}\n",
        "    result = []\n",
        "    for word in tokens:\n",
        "        if word in dict_vectors:\n",
        "            result.append(dict_vectors[word])\n",
        "        else:\n",
        "            dict_vectors[word] = len(dict_vectors)\n",
        "            result.append(dict_vectors[word])\n",
        "    return result"
      ],
      "metadata": {
        "id": "mCC0EFOZU6Cv"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def main():\n",
        "    \"\"\"\n",
        "    Основная функция программы.\n",
        "    \"\"\"\n",
        "    documents = [\n",
        "        \"Far over the misty mountains cold\",\n",
        "        \"To dungeons deep and caverns old\",\n",
        "        \"We must away ere break of day\",\n",
        "        \"To find our long-forgotten gold.\"\n",
        "\n",
        "        \"The pines were roaring on the height\",\n",
        "        \"The winds were moaning in the night\",\n",
        "        \"The fire was red, it flaming spread\",\n",
        "        \"The trees like torches blazed with light.\"\n",
        "    ]\n",
        "\n",
        "    preprocessed_docs = [preprocess_text(doc) for doc in documents]\n",
        "    tokenized_docs = [\" \".join(tokenize_and_lemmatize(doc)) for doc in preprocessed_docs]\n",
        "\n",
        "    print(\"\\nPreprocessed Documents:\", tokenized_docs)\n",
        "\n",
        "    vocab = build_vocabulary(tokenized_docs)\n",
        "\n",
        "    bow_matrix = bag_of_words(tokenized_docs, vocab)\n",
        "    tfidf_matrix = compute_tfidf(bow_matrix)\n",
        "\n",
        "    np.set_printoptions(precision=1, suppress=True)\n",
        "    print(\"\\nBag of Words Matrix:\")\n",
        "    print(bow_matrix)\n",
        "    print(\"\\nTF-IDF Matrix:\")\n",
        "    print(tfidf_matrix)\n",
        "\n",
        "    print(\"\\nASCII Tokenization:\", ascii_tokenizer(documents[0]))\n",
        "    print(\"\\nASCII Vectorization:\", ascii_vectorizer(documents[0]))\n",
        "\n",
        "    print(\"\\nVectorized Tokens (Lemmatized):\", vectorize(tokenize_and_lemmatize(documents[0])))\n",
        "    print(\"\\nVectorized Tokens (Preprocessed):\", vectorize(preprocessed_docs[0].split()))\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lZTn1b_WU-q_",
        "outputId": "d5ca23eb-3042-493f-a1bb-f1798595606b"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Preprocessed Documents: ['far misty mountain cold', 'dungeon deep cavern old', 'must away ere break day', 'find longforgotten goldthe pine roaring height', 'wind moaning night', 'fire red flaming spread', 'tree like torch blazed light']\n",
            "\n",
            "Bag of Words Matrix:\n",
            "[[1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 0. 0.\n",
            "  0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1.\n",
            "  1. 1. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 1. 1. 1. 1. 1.]]\n",
            "\n",
            "TF-IDF Matrix:\n",
            "[[0.2 0.2 0.2 0.2 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
            "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0. ]\n",
            " [0.  0.  0.  0.  0.2 0.2 0.2 0.2 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
            "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0. ]\n",
            " [0.  0.  0.  0.  0.  0.  0.  0.  0.2 0.2 0.2 0.2 0.2 0.  0.  0.  0.  0.\n",
            "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0. ]\n",
            " [0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.1 0.1 0.1 0.1 0.1\n",
            "  0.1 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0. ]\n",
            " [0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
            "  0.  0.3 0.3 0.3 0.  0.  0.  0.  0.  0.  0.  0.  0. ]\n",
            " [0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
            "  0.  0.  0.  0.  0.2 0.2 0.2 0.2 0.  0.  0.  0.  0. ]\n",
            " [0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
            "  0.  0.  0.  0.  0.  0.  0.  0.  0.2 0.2 0.2 0.2 0.2]]\n",
            "\n",
            "ASCII Tokenization: ['F', 'a', 'r', ' ', 'o', 'v', 'e', 'r', ' ', 't', 'h', 'e', ' ', 'm', 'i', 's', 't', 'y', ' ', 'm', 'o', 'u', 'n', 't', 'a', 'i', 'n', 's', ' ', 'c', 'o', 'l', 'd']\n",
            "\n",
            "ASCII Vectorization: [70, 97, 114, 32, 111, 118, 101, 114, 32, 116, 104, 101, 32, 109, 105, 115, 116, 121, 32, 109, 111, 117, 110, 116, 97, 105, 110, 115, 32, 99, 111, 108, 100]\n",
            "\n",
            "Vectorized Tokens (Lemmatized): [0, 1, 2, 3]\n",
            "\n",
            "Vectorized Tokens (Preprocessed): [0, 1, 2, 3, 4, 5]\n"
          ]
        }
      ]
    }
  ]
}