{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gn1dus/mosi/blob/main/%D0%9B%D0%B0%D0%B1%D0%B0_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pymorphy3"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dhq9jqCxR2ng",
        "outputId": "752b625d-13c7-4cef-9aab-9360a3d58d52"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pymorphy3 in /usr/local/lib/python3.11/dist-packages (2.0.3)\n",
            "Requirement already satisfied: dawg2-python>=0.8.0 in /usr/local/lib/python3.11/dist-packages (from pymorphy3) (0.9.0)\n",
            "Requirement already satisfied: pymorphy3-dicts-ru in /usr/local/lib/python3.11/dist-packages (from pymorphy3) (2.4.417150.4580142)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Загружаем библиотеки"
      ],
      "metadata": {
        "id": "zFzpVJWRS_5m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "import pymorphy3\n",
        "from nltk.stem import SnowballStemmer\n",
        "from nltk.tokenize import word_tokenize\n",
        "import string"
      ],
      "metadata": {
        "id": "rL54imCiS-iv"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Загружаем необходимые пакеты"
      ],
      "metadata": {
        "id": "hoAuJxLVTDhO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "nltk.download('punkt_tab')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d8x6FlYHTHGm",
        "outputId": "40f58e02-00e8-495f-8abd-396475b90885"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Инициализация MorphAnalyzer для лемматизации"
      ],
      "metadata": {
        "id": "6WZJQkrJTH2G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "morph3 = pymorphy3.MorphAnalyzer()"
      ],
      "metadata": {
        "id": "HfEfbmIuTKdO"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Функция для лемматизации текста"
      ],
      "metadata": {
        "id": "ViRLa87kTMSn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def lematize(text: str) -> list[str]:\n",
        "    tokens = word_tokenize(text)\n",
        "    lemmatized_words = [morph3.parse(word)[0].normal_form for word in tokens]\n",
        "    return lemmatized_words"
      ],
      "metadata": {
        "id": "yrq6uHHsTOZG"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Функция для стемминга текста"
      ],
      "metadata": {
        "id": "-VO0KAEeTQPe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def stemming(text: str) -> list[str]:\n",
        "    stemmer = SnowballStemmer(\"russian\")\n",
        "    tokens = word_tokenize(text)\n",
        "    stemmed_words = [stemmer.stem(word) for word in tokens]\n",
        "    return stemmed_words"
      ],
      "metadata": {
        "id": "M6jcQQoqTVbO"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Функция для токенизации всех символов из ASCII"
      ],
      "metadata": {
        "id": "Pohjyx7JTZGu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def ascii_tokenizer(text: str) -> list[str]:\n",
        "    return [char for char in text if char in string.printable]"
      ],
      "metadata": {
        "id": "5hts90XPTbDW"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Функция для векторизации всех символов из ASCII"
      ],
      "metadata": {
        "id": "zzJurIcRTdAe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def ascii_vectorizer(text: str) -> list[int]:\n",
        "    return [ord(char) for char in text if char in string.printable]"
      ],
      "metadata": {
        "id": "BE0M66r_Tf0W"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Функция для токенизации текста"
      ],
      "metadata": {
        "id": "PIA90ibdTj3e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize(text: str) -> list[str]:\n",
        "    return word_tokenize(text)"
      ],
      "metadata": {
        "id": "d74UiW-8TlF-"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Функция для векторизации текста"
      ],
      "metadata": {
        "id": "nUyfqenITmnm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def vectorize(tokens: list[str]) -> list[int]:\n",
        "    dict_vectors = {}\n",
        "    result = []\n",
        "    for word in tokens:\n",
        "        if word in dict_vectors:\n",
        "            result.append(dict_vectors[word])\n",
        "        else:\n",
        "            dict_vectors[word] = len(dict_vectors)\n",
        "            result.append(dict_vectors[word])\n",
        "    return result"
      ],
      "metadata": {
        "id": "SyJ4flqKTron"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Текст"
      ],
      "metadata": {
        "id": "EapG70gRTuFu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Основной код\n",
        "text = 'Гарри потянулся к молнии и приоткрыл сумку. Внутри оказался толстый, мягкий рулон денег. — Это твои, — сказал Хагрид, улыбаясь. — На всё первое время хватит. Потом сможешь снять ещё из твоего хранилища в Гринготтсе. — У меня есть хранилище в Гринготтсе? — поразился Гарри.'\n",
        "english_text = 'Harry reached into the bag and pulled out a thick wad of notes. — That’s yours, — said Hagrid, smiling. —Enough fer the basics. Yeh’ll be able ter take money outta yer vault in Gringotts. — I’ve got a vault in Gringotts? — said Harry, amazed.'"
      ],
      "metadata": {
        "id": "9oHnMIPYUM0x"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Лемматизация\n",
        "print('Лемматизация:')\n",
        "print(lematize(text))\n",
        "\n",
        "# Стемминг\n",
        "print('Стемминг:')\n",
        "print(stemming(text))\n",
        "\n",
        "# Токенизация всех символов из ASCII\n",
        "print('Токенизация всех символов из ASCII:')\n",
        "print(ascii_tokenizer(english_text))\n",
        "\n",
        "# Векторизация всех символов из ASCII\n",
        "print('Векторизация всех символов из ASCII:')\n",
        "print(ascii_vectorizer(english_text))\n",
        "\n",
        "# Векторизация текста после лемматизации\n",
        "print('Векторизация текста после лемматизации:')\n",
        "print(vectorize(lematize(text)))\n",
        "\n",
        "# Векторизация текста после стемминга\n",
        "print('Векторизация текста после стемминга:')\n",
        "print(vectorize(stemming(text)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8SXPf7qEUVAe",
        "outputId": "b877473e-d975-4ccc-d689-a8bf4aab371f"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Лемматизация:\n",
            "['гарри', 'потянуться', 'к', 'молния', 'и', 'приоткрыть', 'сумка', '.', 'внутри', 'оказаться', 'толстый', ',', 'мягкий', 'рулон', 'деньга', '.', '—', 'это', 'твой', ',', '—', 'сказать', 'хагрид', ',', 'улыбаться', '.', '—', 'на', 'всё', 'первый', 'время', 'хватить', '.', 'потом', 'смочь', 'снять', 'ещё', 'из', 'твой', 'хранилище', 'в', 'гринготтс', '.', '—', 'у', 'я', 'есть', 'хранилище', 'в', 'гринготтс', '?', '—', 'поразиться', 'гарри', '.']\n",
            "Стемминг:\n",
            "['гарр', 'потянул', 'к', 'молн', 'и', 'приоткр', 'сумк', '.', 'внутр', 'оказа', 'толст', ',', 'мягк', 'рулон', 'денег', '.', '—', 'эт', 'тво', ',', '—', 'сказа', 'хагрид', ',', 'улыб', '.', '—', 'на', 'все', 'перв', 'врем', 'хват', '.', 'пот', 'сможеш', 'снят', 'ещ', 'из', 'тво', 'хранилищ', 'в', 'гринготтс', '.', '—', 'у', 'мен', 'ест', 'хранилищ', 'в', 'гринготтс', '?', '—', 'пораз', 'гарр', '.']\n",
            "Токенизация всех символов из ASCII:\n",
            "['H', 'a', 'r', 'r', 'y', ' ', 'r', 'e', 'a', 'c', 'h', 'e', 'd', ' ', 'i', 'n', 't', 'o', ' ', 't', 'h', 'e', ' ', 'b', 'a', 'g', ' ', 'a', 'n', 'd', ' ', 'p', 'u', 'l', 'l', 'e', 'd', ' ', 'o', 'u', 't', ' ', 'a', ' ', 't', 'h', 'i', 'c', 'k', ' ', 'w', 'a', 'd', ' ', 'o', 'f', ' ', 'n', 'o', 't', 'e', 's', '.', ' ', ' ', 'T', 'h', 'a', 't', 's', ' ', 'y', 'o', 'u', 'r', 's', ',', ' ', ' ', 's', 'a', 'i', 'd', ' ', 'H', 'a', 'g', 'r', 'i', 'd', ',', ' ', 's', 'm', 'i', 'l', 'i', 'n', 'g', '.', ' ', 'E', 'n', 'o', 'u', 'g', 'h', ' ', 'f', 'e', 'r', ' ', 't', 'h', 'e', ' ', 'b', 'a', 's', 'i', 'c', 's', '.', ' ', 'Y', 'e', 'h', 'l', 'l', ' ', 'b', 'e', ' ', 'a', 'b', 'l', 'e', ' ', 't', 'e', 'r', ' ', 't', 'a', 'k', 'e', ' ', 'm', 'o', 'n', 'e', 'y', ' ', 'o', 'u', 't', 't', 'a', ' ', 'y', 'e', 'r', ' ', 'v', 'a', 'u', 'l', 't', ' ', 'i', 'n', ' ', 'G', 'r', 'i', 'n', 'g', 'o', 't', 't', 's', '.', ' ', ' ', 'I', 'v', 'e', ' ', 'g', 'o', 't', ' ', 'a', ' ', 'v', 'a', 'u', 'l', 't', ' ', 'i', 'n', ' ', 'G', 'r', 'i', 'n', 'g', 'o', 't', 't', 's', '?', ' ', ' ', 's', 'a', 'i', 'd', ' ', 'H', 'a', 'r', 'r', 'y', ',', ' ', 'a', 'm', 'a', 'z', 'e', 'd', '.']\n",
            "Векторизация всех символов из ASCII:\n",
            "[72, 97, 114, 114, 121, 32, 114, 101, 97, 99, 104, 101, 100, 32, 105, 110, 116, 111, 32, 116, 104, 101, 32, 98, 97, 103, 32, 97, 110, 100, 32, 112, 117, 108, 108, 101, 100, 32, 111, 117, 116, 32, 97, 32, 116, 104, 105, 99, 107, 32, 119, 97, 100, 32, 111, 102, 32, 110, 111, 116, 101, 115, 46, 32, 32, 84, 104, 97, 116, 115, 32, 121, 111, 117, 114, 115, 44, 32, 32, 115, 97, 105, 100, 32, 72, 97, 103, 114, 105, 100, 44, 32, 115, 109, 105, 108, 105, 110, 103, 46, 32, 69, 110, 111, 117, 103, 104, 32, 102, 101, 114, 32, 116, 104, 101, 32, 98, 97, 115, 105, 99, 115, 46, 32, 89, 101, 104, 108, 108, 32, 98, 101, 32, 97, 98, 108, 101, 32, 116, 101, 114, 32, 116, 97, 107, 101, 32, 109, 111, 110, 101, 121, 32, 111, 117, 116, 116, 97, 32, 121, 101, 114, 32, 118, 97, 117, 108, 116, 32, 105, 110, 32, 71, 114, 105, 110, 103, 111, 116, 116, 115, 46, 32, 32, 73, 118, 101, 32, 103, 111, 116, 32, 97, 32, 118, 97, 117, 108, 116, 32, 105, 110, 32, 71, 114, 105, 110, 103, 111, 116, 116, 115, 63, 32, 32, 115, 97, 105, 100, 32, 72, 97, 114, 114, 121, 44, 32, 97, 109, 97, 122, 101, 100, 46]\n",
            "Векторизация текста после лемматизации:\n",
            "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 7, 15, 16, 17, 11, 15, 18, 19, 11, 20, 7, 15, 21, 22, 23, 24, 25, 7, 26, 27, 28, 29, 30, 17, 31, 32, 33, 7, 15, 34, 35, 36, 31, 32, 33, 37, 15, 38, 0, 7]\n",
            "Векторизация текста после стемминга:\n",
            "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 7, 15, 16, 17, 11, 15, 18, 19, 11, 20, 7, 15, 21, 22, 23, 24, 25, 7, 26, 27, 28, 29, 30, 17, 31, 32, 33, 7, 15, 34, 35, 36, 31, 32, 33, 37, 15, 38, 0, 7]\n"
          ]
        }
      ]
    }
  ]
}